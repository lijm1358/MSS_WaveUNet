{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijm1358/anaconda3/envs/soundprocessing/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import librosa\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MUSDBDataset(Dataset):\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.crop_size = 284672\n",
    "        self.data_dir = os.path.join(data_dir, 'data_numpy')\n",
    "        if not os.path.exists(self.data_dir) or \\\n",
    "            len([name for name in os.listdir(self.data_dir)]) < 500:\n",
    "            print(\"Data has not been saved as numpy object. Converting...\")\n",
    "            if not os.path.exists(self.data_dir):\n",
    "                os.makedirs(self.data_dir)\n",
    "            self.convert_to_numpy(data_dir, self.data_dir)\n",
    "        self.music_fulllist = self.get_filenames(self.data_dir)\n",
    "        self.music_list, self.sep_list = self.separate_source(self.music_fulllist)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.music_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_music = self.music_list[idx]\n",
    "        base_music = np.load(base_music)\n",
    "        base_music = np.stack([base_music[:self.crop_size]])\n",
    "\n",
    "        sep_music = self.sep_list[idx*4 : idx*4+4]\n",
    "        sep_music = np.stack([np.load(idx)[:self.crop_size] for idx in sep_music])\n",
    "        return base_music, sep_music\n",
    "\n",
    "    def get_filenames(self, path):\n",
    "        files_list = list()\n",
    "        for filename in os.listdir(path):\n",
    "            if not filename == \"data_numpy\":\n",
    "                files_list.append(os.path.join(path, filename))\n",
    "        return files_list\n",
    "\n",
    "    def convert_to_numpy(self, music_dir, target_dir):\n",
    "        warnings.filterwarnings('ignore')\n",
    "        music_list = self.get_filenames(music_dir)\n",
    "        for music in tqdm(music_list):\n",
    "            outfile_name = music.split(\"/\")[-1]\n",
    "            outfile_name = target_dir + \"/\" + outfile_name\n",
    "            arr, _ = librosa.load(music)\n",
    "            np.save(outfile_name, arr)\n",
    "\n",
    "    def separate_source(self, mus_list):\n",
    "        warnings.filterwarnings('ignore')\n",
    "        music_list = list()\n",
    "        sep_list = list()\n",
    "        for music in tqdm(mus_list):\n",
    "            mus_type = music.split(\".\")[-3]\n",
    "            if mus_type == '0':\n",
    "                music_list.append(music)\n",
    "            else:\n",
    "                sep_list.append(music)\n",
    "\n",
    "        return music_list, sep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1364445.02it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = MUSDBDataset('/mnt/d/createdmusdb18/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(ds, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DownSampling(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=24, kernel_size=15):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=7),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=7),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampling(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, x_back):\n",
    "        x = self.upsample(x);\n",
    "        x = torch.cat([x, x_back], axis=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveUNet(nn.Module):\n",
    "    def __init__(self, n_level=12, n_source=4):\n",
    "        super().__init__()\n",
    "        self.level = n_level\n",
    "        \n",
    "        self.layer_to_concat = []\n",
    "        \n",
    "        layers=[DownSampling(in_ch=1,out_ch=24,kernel_size=15)]\n",
    "        \n",
    "        for i in range(self.level-1):\n",
    "            layers.append(DownSampling(in_ch=24*(i+1),out_ch=24*(i+2),kernel_size=15))\n",
    "            \n",
    "        # layers.append(DownSampling(in_ch=24*(self.level), out_ch=24*(self.level+1), kernel_size=15, decimate=False))\n",
    "        layers.append(DownSampling(in_ch=24*(self.level), out_ch=24*(self.level+1), kernel_size=15))\n",
    "            \n",
    "        for i in range(self.level):\n",
    "            layers.append(UpSampling(in_ch=24*(self.level+1-i) + 24*(self.level - i), out_ch=24*(self.level-i), kernel_size=5))\n",
    "            \n",
    "        self.net = nn.ModuleList(layers)\n",
    "        self.separation = nn.Sequential(\n",
    "            nn.Conv1d(25, n_source, kernel_size=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv1d(n_source, n_source, kernel_size=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        print(\"before in \", x.shape)\n",
    "        self.layer_to_concat.append(x)\n",
    "        for layer in self.net[0: self.level]:\n",
    "            x = layer(x)\n",
    "            print(\"conv \", x.shape)\n",
    "            self.layer_to_concat.append(x);\n",
    "            x = x[:, :, 1::2]\n",
    "            print(\"decimate \", x.shape)\n",
    "        x = self.net[self.level](x)\n",
    "        print(\"middle out \", x.shape)\n",
    "        self.layer_to_concat.append(x); \n",
    "        for i, layer in enumerate(self.net[self.level+1:]):\n",
    "            print(\"before up \", x.shape)\n",
    "            x = self.layer_to_concat[-1];\n",
    "            x = layer(x, self.layer_to_concat[-1-i-1])\n",
    "            print(\"after up \", x.shape)\n",
    "            self.layer_to_concat[-1] = x\n",
    "            \n",
    "        x = torch.cat([self.layer_to_concat[0], x], axis=1)\n",
    "        x = self.separation(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.rand((1, 1, 16384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before in  torch.Size([1, 1, 16384])\n",
      "conv  torch.Size([1, 24, 16384])\n",
      "decimate  torch.Size([1, 24, 8192])\n",
      "conv  torch.Size([1, 48, 8192])\n",
      "decimate  torch.Size([1, 48, 4096])\n",
      "conv  torch.Size([1, 72, 4096])\n",
      "decimate  torch.Size([1, 72, 2048])\n",
      "conv  torch.Size([1, 96, 2048])\n",
      "decimate  torch.Size([1, 96, 1024])\n",
      "conv  torch.Size([1, 120, 1024])\n",
      "decimate  torch.Size([1, 120, 512])\n",
      "conv  torch.Size([1, 144, 512])\n",
      "decimate  torch.Size([1, 144, 256])\n",
      "conv  torch.Size([1, 168, 256])\n",
      "decimate  torch.Size([1, 168, 128])\n",
      "conv  torch.Size([1, 192, 128])\n",
      "decimate  torch.Size([1, 192, 64])\n",
      "conv  torch.Size([1, 216, 64])\n",
      "decimate  torch.Size([1, 216, 32])\n",
      "conv  torch.Size([1, 240, 32])\n",
      "decimate  torch.Size([1, 240, 16])\n",
      "conv  torch.Size([1, 264, 16])\n",
      "decimate  torch.Size([1, 264, 8])\n",
      "conv  torch.Size([1, 288, 8])\n",
      "decimate  torch.Size([1, 288, 4])\n",
      "middle out  torch.Size([1, 312, 4])\n",
      "before up  torch.Size([1, 312, 4])\n",
      "after up  torch.Size([1, 288, 8])\n",
      "before up  torch.Size([1, 288, 8])\n",
      "after up  torch.Size([1, 264, 16])\n",
      "before up  torch.Size([1, 264, 16])\n",
      "after up  torch.Size([1, 240, 32])\n",
      "before up  torch.Size([1, 240, 32])\n",
      "after up  torch.Size([1, 216, 64])\n",
      "before up  torch.Size([1, 216, 64])\n",
      "after up  torch.Size([1, 192, 128])\n",
      "before up  torch.Size([1, 192, 128])\n",
      "after up  torch.Size([1, 168, 256])\n",
      "before up  torch.Size([1, 168, 256])\n",
      "after up  torch.Size([1, 144, 512])\n",
      "before up  torch.Size([1, 144, 512])\n",
      "after up  torch.Size([1, 120, 1024])\n",
      "before up  torch.Size([1, 120, 1024])\n",
      "after up  torch.Size([1, 96, 2048])\n",
      "before up  torch.Size([1, 96, 2048])\n",
      "after up  torch.Size([1, 72, 4096])\n",
      "before up  torch.Size([1, 72, 4096])\n",
      "after up  torch.Size([1, 48, 8192])\n",
      "before up  torch.Size([1, 48, 8192])\n",
      "after up  torch.Size([1, 24, 16384])\n"
     ]
    }
   ],
   "source": [
    "model = WaveUNet(n_level=12, n_source=4)\n",
    "\n",
    "# out = model(torch.Tensor(ds[0][0]))\n",
    "out = model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 16384])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('soundprocessing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5bbe51b3405e60c087d18985c6a5d36133bee8e93b3430261fcdb872e4da9e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
