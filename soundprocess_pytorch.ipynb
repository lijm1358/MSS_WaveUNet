{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "7FzflK4fL2ll",
        "outputId": "2e7abdad-c999-4585-f6c8-8752122c9dc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "id": "I0s1QHuPL3qr",
        "outputId": "d13f270d-0183-4346-fd64-5bebbc210818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "94kdUPk-LzsA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import subprocess\n",
        "from subprocess import call\n",
        "from functools import partial\n",
        "\n",
        "import librosa\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pydub "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mHRoNfiqLzsC"
      },
      "outputs": [],
      "source": [
        "# path_folder_in = '/content/gdrive/MyDrive/musdb18/test'\n",
        "# path_folder_out = '/content/gdrive/MyDrive/createdmusdb18/test'\n",
        "# \n",
        "# flag = False\n",
        "# \n",
        "# \n",
        "# files = os.listdir(path_folder_in)\n",
        "# for file in tqdm(files, position=0, leave=True):\n",
        "#   # tqdm.write(f\"current song : {file}\")\n",
        "#   if flag == False and file[:5] == \"Nerve\":\n",
        "#     flag = True\n",
        "#   elif flag == False:\n",
        "#     continue\n",
        "#   file_in = path_folder_in + '/' + file\n",
        "#   for i in range(5):\n",
        "#     filesp = file.split(\".\")\n",
        "#     filesp[-1] = f\"{i}.mp3\"\n",
        "#     filesp = \".\".join(filesp)\n",
        "#     file_out = f\"{path_folder_out}/{filesp}\"\n",
        "#     # tqdm.write(f\"processing {file_out}\", end='\\r')\n",
        "#     call(('ffmpeg', '-y', '-i', file_in, '-map', f'0:{i}', '-vn', file_out),\n",
        "#          stdout=subprocess.DEVNULL,\n",
        "#          stderr=subprocess.STDOUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X7N0hkaCLzsD"
      },
      "outputs": [],
      "source": [
        "class MUSDBDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, train:bool=True):\n",
        "        self.data_size = 500 if train is True else 250\n",
        "        self.crop_size = 284672\n",
        "        self.data_dir = os.path.join(data_dir, 'data_numpy')\n",
        "        if not os.path.exists(self.data_dir) or \\\n",
        "            len([name for name in os.listdir(self.data_dir)]) < self.data_size:\n",
        "            print(\"Data has not been saved as numpy object. Converting...\")\n",
        "            if not os.path.exists(self.data_dir):\n",
        "                os.makedirs(self.data_dir)\n",
        "            self.convert_to_numpy(data_dir, self.data_dir)\n",
        "        self.music_fulllist = self.get_filenames(self.data_dir)\n",
        "        self.music_list, self.sep_list = self.separate_source(self.music_fulllist)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.music_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_music = self.music_list[idx]\n",
        "        base_music = np.load(base_music)\n",
        "        base_music = np.stack([base_music[:self.crop_size]])\n",
        "\n",
        "        sep_music = self.sep_list[idx*4 : idx*4+4]\n",
        "        sep_music = np.stack([np.load(idx)[:self.crop_size] for idx in sep_music])\n",
        "        return base_music, sep_music\n",
        "\n",
        "    def get_filenames(self, path):\n",
        "        files_list = list()\n",
        "        for filename in os.listdir(path):\n",
        "            if not filename == \"data_numpy\":\n",
        "                files_list.append(os.path.join(path, filename))\n",
        "        return files_list\n",
        "\n",
        "    def convert_to_numpy(self, music_dir, target_dir):\n",
        "        warnings.filterwarnings('ignore')\n",
        "        music_list = self.get_filenames(music_dir)\n",
        "        for music in tqdm(music_list):\n",
        "            outfile_name = music.split(\"/\")[-1]\n",
        "            outfile_name = target_dir + \"/\" + outfile_name\n",
        "            arr, _ = librosa.load(music)\n",
        "            np.save(outfile_name, arr)\n",
        "\n",
        "    def separate_source(self, mus_list):\n",
        "        warnings.filterwarnings('ignore')\n",
        "        music_list = list()\n",
        "        sep_list = list()\n",
        "        for music in tqdm(mus_list):\n",
        "            mus_type = music.split(\".\")[-3]\n",
        "            if mus_type == '0':\n",
        "                music_list.append(music)\n",
        "            else:\n",
        "                sep_list.append(music)\n",
        "\n",
        "        return music_list, sep_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "htj5Z8MpLzsE",
        "outputId": "58316902-dd30-48b7-848d-1448bdd1c695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 113200.48it/s]\n",
            "100%|██████████| 250/250 [00:00<00:00, 509017.48it/s]\n"
          ]
        }
      ],
      "source": [
        "train_ds = MUSDBDataset('/content/gdrive/MyDrive/createdmusdb18/train')\n",
        "test_ds = MUSDBDataset('/content/gdrive/MyDrive/createdmusdb18/test', train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0XBPVV1sLzsF"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_ds, batch_size=8)\n",
        "# test_dataloader = DataLoader(test_ds, batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3m3_EPkjLzsF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DownSampling(nn.Module):\n",
        "    def __init__(self, in_ch=1, out_ch=24, kernel_size=15):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=7),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=7),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x: Tensor):\n",
        "        x = self.net(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "khmjRe9-LzsG"
      },
      "outputs": [],
      "source": [
        "class UpSampling(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"linear\", align_corners=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=2),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=2),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, x_back):\n",
        "        x = self.upsample(x);\n",
        "        diff = x_back.shape[-1] - x.shape[-1]\n",
        "        x = pad(x, (0, diff))\n",
        "        x = torch.cat([x, x_back], axis=1)\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LOe36a_mLzsH"
      },
      "outputs": [],
      "source": [
        "class WaveUNet(nn.Module):\n",
        "    def __init__(self, n_level=12, n_source=4):\n",
        "        super().__init__()\n",
        "        self.level = n_level\n",
        "        \n",
        "        layers=[DownSampling(in_ch=1,out_ch=24,kernel_size=15)]\n",
        "        \n",
        "        for i in range(self.level-1):\n",
        "            layers.append(DownSampling(in_ch=24*(i+1),out_ch=24*(i+2),kernel_size=15))\n",
        "            \n",
        "        # layers.append(DownSampling(in_ch=24*(self.level), out_ch=24*(self.level+1), kernel_size=15, decimate=False))\n",
        "        layers.append(DownSampling(in_ch=24*(self.level), out_ch=24*(self.level+1), kernel_size=15))\n",
        "            \n",
        "        for i in range(self.level):\n",
        "            layers.append(UpSampling(in_ch=24*(self.level+1-i) + 24*(self.level - i), out_ch=24*(self.level-i), kernel_size=5))\n",
        "            \n",
        "        self.net = nn.ModuleList(layers)\n",
        "        self.separation = nn.Sequential(\n",
        "            nn.Conv1d(25, n_source, kernel_size=1),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv1d(n_source, n_source, kernel_size=1),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: Tensor):\n",
        "        layer_to_concat = []\n",
        "        #print(\"before in \", x.shape)\n",
        "        layer_to_concat.append(x)\n",
        "        for layer in self.net[0: self.level]:\n",
        "            x = layer(x)\n",
        "            #print(\"conv \", x.shape)\n",
        "            layer_to_concat.append(x)\n",
        "            x = x[:, :, 1::2]\n",
        "            #print(\"decimate \", x.shape)\n",
        "        x = self.net[self.level](x)\n",
        "        #print(\"middle out \", x.shape)\n",
        "        layer_to_concat.append(x)\n",
        "        for i, layer in enumerate(self.net[self.level+1:]):\n",
        "            #print(\"before up \", x.shape)\n",
        "            x = layer_to_concat[-1]\n",
        "            x = layer(x, layer_to_concat[-1-i-1])\n",
        "            #print(\"after up \", x.shape)\n",
        "            layer_to_concat[-1] = x\n",
        "            \n",
        "        x = torch.cat([layer_to_concat[0], x], axis=1)\n",
        "        x = self.separation(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rwTtpsAdLzsH",
        "outputId": "92b608c3-fc8d-47ab-87c4-d85fae016f0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wJcJkej6LzsI"
      },
      "outputs": [],
      "source": [
        "model = WaveUNet(n_level=12).to(device)\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "L0646__YLzsI"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=[0.9, 0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p9ikop1ULzsJ"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        print(f\"loss : {loss.item()} ({batch*len(X)}/{size})\")\n",
        "\n",
        "        del pred\n",
        "        del loss\n",
        "        del X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3MhFHqZwLzsJ",
        "outputId": "5d117497-0501-4604-f329-6c51aaebe0e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 1\n",
            "---------------------------\n",
            "loss : 0.001883947174064815 (0/100)\n",
            "loss : 0.0016691356431692839 (8/100)\n",
            "loss : 0.0009247639682143927 (16/100)\n",
            "loss : 0.0007828031666576862 (24/100)\n",
            "loss : 0.0015178773319348693 (32/100)\n",
            "loss : 0.001023075426928699 (40/100)\n",
            "loss : 0.002017098944634199 (48/100)\n",
            "loss : 0.005202786065638065 (56/100)\n",
            "loss : 0.0026227342896163464 (64/100)\n",
            "loss : 0.0016260803677141666 (72/100)\n",
            "loss : 0.0014834179310128093 (80/100)\n",
            "loss : 0.0013023486826568842 (88/100)\n",
            "loss : 0.0020757457241415977 (48/100)\n",
            "epoch : 2\n",
            "---------------------------\n",
            "loss : 0.0018781651742756367 (0/100)\n",
            "loss : 0.00166327937040478 (8/100)\n",
            "loss : 0.0009210366988554597 (16/100)\n",
            "loss : 0.0007796613499522209 (24/100)\n",
            "loss : 0.0015142858028411865 (32/100)\n",
            "loss : 0.001020219293422997 (40/100)\n",
            "loss : 0.0020143596921116114 (48/100)\n",
            "loss : 0.005200695712119341 (56/100)\n",
            "loss : 0.002621300518512726 (64/100)\n",
            "loss : 0.001624582102522254 (72/100)\n",
            "loss : 0.0014819740317761898 (80/100)\n",
            "loss : 0.0013011499540880322 (88/100)\n",
            "loss : 0.002074602758511901 (48/100)\n",
            "epoch : 3\n",
            "---------------------------\n",
            "loss : 0.001877551549114287 (0/100)\n",
            "loss : 0.001662369933910668 (8/100)\n",
            "loss : 0.0009201455395668745 (16/100)\n",
            "loss : 0.0007790426607243717 (24/100)\n",
            "loss : 0.0015135265421122313 (32/100)\n",
            "loss : 0.0010196298826485872 (40/100)\n",
            "loss : 0.0020132914651185274 (48/100)\n",
            "loss : 0.005198832601308823 (56/100)\n",
            "loss : 0.0026201216969639063 (64/100)\n",
            "loss : 0.0016228343592956662 (72/100)\n",
            "loss : 0.0014808302512392402 (80/100)\n",
            "loss : 0.0013005445944145322 (88/100)\n",
            "loss : 0.0020729389507323503 (48/100)\n",
            "epoch : 4\n",
            "---------------------------\n",
            "loss : 0.0018757946090772748 (0/100)\n",
            "loss : 0.0016599850496277213 (8/100)\n",
            "loss : 0.0009196556056849658 (16/100)\n",
            "loss : 0.0007786067435517907 (24/100)\n",
            "loss : 0.0015117545844987035 (32/100)\n",
            "loss : 0.0010189396562054753 (40/100)\n",
            "loss : 0.002011216478422284 (48/100)\n",
            "loss : 0.005195059347897768 (56/100)\n",
            "loss : 0.002618062077090144 (64/100)\n",
            "loss : 0.0016200115205720067 (72/100)\n",
            "loss : 0.0014789275592193007 (80/100)\n",
            "loss : 0.0012994235148653388 (88/100)\n",
            "loss : 0.0020703275222331285 (48/100)\n",
            "epoch : 5\n",
            "---------------------------\n",
            "loss : 0.0018733551260083914 (0/100)\n",
            "loss : 0.0016563747776672244 (8/100)\n",
            "loss : 0.0009182665962725878 (16/100)\n",
            "loss : 0.0007775467238388956 (24/100)\n",
            "loss : 0.0015091551467776299 (32/100)\n",
            "loss : 0.001017607981339097 (40/100)\n",
            "loss : 0.0020081298425793648 (48/100)\n",
            "loss : 0.005189943592995405 (56/100)\n",
            "loss : 0.0026153256185352802 (64/100)\n",
            "loss : 0.0016162145184352994 (72/100)\n",
            "loss : 0.0014764044899493456 (80/100)\n",
            "loss : 0.0012980739120393991 (88/100)\n",
            "loss : 0.0020665707997977734 (48/100)\n",
            "epoch : 6\n",
            "---------------------------\n",
            "loss : 0.0018701193621382117 (0/100)\n",
            "loss : 0.001651458558626473 (8/100)\n",
            "loss : 0.0009167909738607705 (16/100)\n",
            "loss : 0.0007764920010231435 (24/100)\n",
            "loss : 0.0015055508119985461 (32/100)\n",
            "loss : 0.0010160286910831928 (40/100)\n",
            "loss : 0.0020037093199789524 (48/100)\n",
            "loss : 0.005182771477848291 (56/100)\n",
            "loss : 0.0026115444488823414 (64/100)\n",
            "loss : 0.0016109751304611564 (72/100)\n",
            "loss : 0.0014727876987308264 (80/100)\n",
            "loss : 0.0012959919404238462 (88/100)\n",
            "loss : 0.0020610333885997534 (48/100)\n",
            "epoch : 7\n",
            "---------------------------\n",
            "loss : 0.0018656968604773283 (0/100)\n",
            "loss : 0.0016444599023088813 (8/100)\n",
            "loss : 0.0009139467729255557 (16/100)\n",
            "loss : 0.000774532207287848 (24/100)\n",
            "loss : 0.0015003938460722566 (32/100)\n",
            "loss : 0.001013460336253047 (40/100)\n",
            "loss : 0.001997057581320405 (48/100)\n",
            "loss : 0.005172915756702423 (56/100)\n",
            "loss : 0.002606229158118367 (64/100)\n",
            "loss : 0.0016038918402045965 (72/100)\n",
            "loss : 0.0014679459854960442 (80/100)\n",
            "loss : 0.0012933950638398528 (88/100)\n",
            "loss : 0.002053141128271818 (48/100)\n",
            "epoch : 8\n",
            "---------------------------\n",
            "loss : 0.0018601786578074098 (0/100)\n",
            "loss : 0.0016357054701074958 (8/100)\n",
            "loss : 0.0009109526290558279 (16/100)\n",
            "loss : 0.0007727009942755103 (24/100)\n",
            "loss : 0.001494507072493434 (32/100)\n",
            "loss : 0.00101079058367759 (40/100)\n",
            "loss : 0.001989488722756505 (48/100)\n",
            "loss : 0.005161965265870094 (56/100)\n",
            "loss : 0.0026002877857536077 (64/100)\n",
            "loss : 0.0015962660545483232 (72/100)\n",
            "loss : 0.0014627112541347742 (80/100)\n",
            "loss : 0.0012907115742564201 (88/100)\n",
            "loss : 0.0020453212782740593 (48/100)\n",
            "epoch : 9\n",
            "---------------------------\n",
            "loss : 0.001854454050771892 (0/100)\n",
            "loss : 0.0016268430044874549 (8/100)\n",
            "loss : 0.0009074059198610485 (16/100)\n",
            "loss : 0.0007703296141698956 (24/100)\n",
            "loss : 0.0014885016717016697 (32/100)\n",
            "loss : 0.0010077086044475436 (40/100)\n",
            "loss : 0.0019815394189208746 (48/100)\n",
            "loss : 0.005150279030203819 (56/100)\n",
            "loss : 0.002593574346974492 (64/100)\n",
            "loss : 0.0015878400299698114 (72/100)\n",
            "loss : 0.0014569363556802273 (80/100)\n",
            "loss : 0.0012880013091489673 (88/100)\n",
            "loss : 0.0020365098025649786 (48/100)\n",
            "epoch : 10\n",
            "---------------------------\n",
            "loss : 0.001848042942583561 (0/100)\n",
            "loss : 0.0016170345479622483 (8/100)\n",
            "loss : 0.0009035093244165182 (16/100)\n",
            "loss : 0.000767399906180799 (24/100)\n",
            "loss : 0.0014818916097283363 (32/100)\n",
            "loss : 0.00100414187181741 (40/100)\n",
            "loss : 0.0019728397019207478 (48/100)\n",
            "loss : 0.005136603489518166 (56/100)\n",
            "loss : 0.0025853051338344812 (64/100)\n",
            "loss : 0.0015776229556649923 (72/100)\n",
            "loss : 0.001450117095373571 (80/100)\n",
            "loss : 0.0012853287626057863 (88/100)\n",
            "loss : 0.0020263686310499907 (48/100)\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"epoch : {t+1}\\n---------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "g8T2O9C5LzsJ"
      },
      "outputs": [],
      "source": [
        "splittest = torch.Tensor(train_ds[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8veJXS_QLzsJ",
        "outputId": "e7a69b3d-4f60-431b-a89a-d9ed0c5d47d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9e119a9981f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplittest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-97d3a07cbc54>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mlayer_to_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;31m#print(\"conv \", x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mlayer_to_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5ead959075ec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    307\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 309\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    310\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(splittest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_GFvF0QLzsK"
      },
      "outputs": [],
      "source": [
        "def write(f, sr, x, normalized=False):\n",
        "    \"\"\"numpy array to MP3\"\"\"\n",
        "    channels = 2 if (x.ndim == 2 and x.shape[1] == 2) else 1\n",
        "    if normalized:  # normalized array - each item should be a float in [-1, 1)\n",
        "        y = np.int16(x * 2 ** 15)\n",
        "    else:\n",
        "        y = np.int16(x)\n",
        "    song = pydub.AudioSegment(y.tobytes(), frame_rate=sr, sample_width=2, channels=channels)\n",
        "    song.export(f, format=\"mp3\", bitrate=\"320k\")\n",
        "\n",
        "write('/content/gdrive/MyDrive/test.mp3', 22050, np.array(splittest[0][0].cpu()), normalized=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 ('soundprocessing')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a5bbe51b3405e60c087d18985c6a5d36133bee8e93b3430261fcdb872e4da9e2"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}